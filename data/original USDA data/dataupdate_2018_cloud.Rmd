---
title: 'SNAP Retailers: Data update November 2018'
author: "Jerry Shannon"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())
getwd()
library(tidyverse)
library(stringdist)
```

## Loading data

We first load the original dataset, compiled in mid 2018. Then we will load data for retailers authorized for SNAP on June 30, 2018. When reading these in, we convert store names, address, and cities to lower case and remove all punctuation to facilitate matching.

Code for punctuation removal: https://stackoverflow.com/questions/29098801/removing-punctuations-from-text-using-r
Code for making sure all characters are UTF-8 for comparison (mutate_at below):  (https://stackoverflow.com/questions/17291287/how-to-identify-delete-non-utf-8-characters-in-r)

```{r loadfiles, message=FALSE}
stype_crosswalk<-read_csv("https://github.com/jshannon75/snap_retailers_2008_2017/raw/master/data/snap_retailers_crosswalk.csv")

st_data<-read_csv("https://github.com/jshannon75/snap_retailers_2008_2017/raw/master/data/snap_retailers_usda.csv") %>%
  left_join(stype_crosswalk) %>%
  mutate_at(vars(store_name,addr_st,city), function(x){gsub('[^ -~]', '',x)}) %>% 
  mutate(store_name_m=tolower(store_name),
         addr_st_m=tolower(addr_st),
         city_m=tolower(city)) %>%
  mutate_at(vars(store_name_m,addr_st_m,city_m), function(x){gsub("[[:punct:][:blank:]]+", " ",x)}) 

june2018<-read_csv("data/original USDA data/SNAP Authorized Stores June 2018.csv") %>%
  select(st_type,store_name,addr_num,addr_st,city,state,zip5,X,Y) %>%
  distinct()%>%
  group_by(store_name,addr_num,addr_st,city,state,zip5)%>%
  summarise(st_type=first(st_type),#Some stores listed multiple times with different types
            X=max(X),
            Y=max(Y)) %>% 
  mutate(Y2018=1) 
june18id<-data.frame(1:nrow(june2018))
names(june18id)<-"june18id"
june2018<-bind_cols(june2018,june18id) %>%
  left_join(stype_crosswalk) %>%
  ungroup()%>%
  mutate_at(vars(store_name), function(x){gsub('[^ -~]', '', x)}) %>% 
  mutate(june18_c=paste("S",june18id),
         method="usda",
         store_name_m=tolower(store_name),
         addr_st_m=tolower(addr_st),
         city_m=tolower(city)) %>%
  mutate_at(vars(store_name_m,addr_st_m,city_m), function(x){gsub("[[:punct:][:blank:]]+", " ", x)})
```

## Dealing with duplicates

The original dataset had a handful (~2,000) of duplicates. This happened when a store changed classification during the study period, a suite number was added, or when the lat/long coordinates changed slightly. We will correct for this by taking the maximum lat and long coordinates and the suite id. For now, we are leaving the duplicates for store identifier in, as there's no overlap in years.

```{r}
st_data_latlong<-st_data %>%
  group_by(storeid) %>%
  mutate(X=max(X),
         Y=max(Y),
         addr_add=first(addr_add)) %>%
  distinct()
```

## Adding 2018 data

We can now try to match a store ID to the 2018 records based on the store name, street name, city, and state. The code below identifies exact matches on address.

```{r}
#Create list from prior years for joining
st_data_join<-st_data_latlong %>%
  select(store_name_m,storeid,addr_num,addr_st_m,city_m,state)%>%
  mutate(addr_num=as.character(addr_num))

#Join to June 2018 data
june2018_stid<-june2018%>%
  left_join(st_data_join) %>%
  distinct()
```

I can then use fuzzy matching on store name to identify additional matches. This last part is memory intensive, so I run it on Google Cloud. In many cases, the differences are about added characters (a location number) or small shifts in the name (added space or punctuation). Family Dollar locations, for example, go from "Family Dollar Store" to just "Family Dollar". Around a distance of 9 or 10, there tend to be actual renamings happening at the same location, such as E-Z shop becoming Enmarket or Deals becoming a Dollar Tree. To preserve those, I use a cutoff of 8 and below for matching.

```{r}
#Generate list of existing store ids that have been matched
stid_match<-june2018_stid %>%
  ungroup() %>%
  filter(is.na(storeid)==FALSE) %>%
  select(storeid) %>% 
  distinct()

#Generate list of 2018 stores that did NOT get matched
june2018_stid_nomatch<-june2018_stid %>%
  filter(is.na(storeid)==TRUE) %>%
  mutate(match=paste(addr_num,addr_st_m,city_m)) #Create match field based on address

#Generate list from existing store database of stores not matched
st_data_nomatch<-st_data_join %>%
  anti_join(stid_match) %>%
  mutate(match=paste(addr_num,addr_st_m,city_m)) %>%
  select(store_name_m,storeid,match) %>%
  rename(store_name_o=store_name_m)

#Match stores based on address. 
fuzzy_join<-june2018_stid_nomatch %>%
  select(-storeid) %>%
  left_join(st_data_nomatch,by="match") %>%
  filter(is.na(storeid)==FALSE) %>%
  ungroup()%>%
  mutate_at(vars(store_name,store_name_o), function(x){gsub('[^ -~]', '', x)}) %>% 
  mutate(name_dist=stringdist(store_name_m,store_name_o))

#Compare names and distance
fuzzy_join1<-fuzzy_join %>%
  select(store_name_m,store_name_o,match,name_dist)

#Keep records with distance of 8 or less for name distance. 
fuzzy_join_select<-fuzzy_join %>%
  filter(name_dist<9) %>%
  select(june18_c,storeid) %>%
  left_join(june2018) %>%
  distinct()

#Join the exact and fuzzy matched records
june2018_matched<-june2018_stid %>%
  filter(is.na(storeid)==FALSE) %>%
  bind_rows(fuzzy_join_select) %>%
  select(-store_name_m,-addr_st_m,-city_m) 
```

We also tried fuzzy matching based on exact store name and approximate address, but did not find any records that fit this criterion.

We can now join these new records into the existing dataset. We do so using bind_rows. We also remove the matching variables and 2018 IDs as well as making sure all year dummy variables are 0 or 1

```{r}
stores_newlist<-st_data_latlong %>%
  anti_join(june2018_matched,by="storeid") %>%
  mutate(addr_num=as.character(addr_num),
         zip5=as.character(zip5)) %>% 
  bind_rows(june2018_matched) %>%
  select(-store_name_m,-addr_st_m,-city_m,-june18id,-june18_c) 
```

We can also then add non-matched addresses as new observations. We start with the highest storeid number in the existing dataset and add from there.

```{r}
nonmatched<-june2018_stid %>%
  anti_join(june2018_matched,by="june18id")

stid_max<-as.numeric(substr(max(stores_newlist$storeid),4,9))
nonmatched$storeid<-paste("st_",(1:nrow(nonmatched)+stid_max),sep="")

stores_newlist_all<-stores_newlist %>%
  bind_rows(nonmatched) %>%
  select(-store_name_m,-addr_st_m,-city_m,-june18id,-june18_c) %>%
  mutate_at(vars(Y2008,Y2009,Y2010,Y2011,Y2012,Y2013,Y2014,Y2015,Y2016,Y2017,Y2018),
            function(x){replace_na(x,0)}) %>%
  select(store_name:Y2017,Y2018,year_first:st_type)
```

We then recalculate the year_first and year_last columns.

```{r}
stores_newlist_year<-stores_newlist_all %>%
  select(storeid,Y2008:Y2018) %>%
  gather(Y2008:Y2018,key="year",value="pres") %>%
  filter(pres==1) %>%
  mutate(year_num=as.numeric(substr(year,2,5))) %>%
  group_by(storeid) %>%
  summarise(year_first=min(year_num),
            year_last=max(year_num)) 

stores_newlist_allyr<-stores_newlist_all %>%
  select(-year_first,-year_last) %>%
  left_join(stores_newlist_year) %>%
  select(store_name:Y2018,year_first,year_last,X:st_type)
```

Lastly, we create a location ID for each unique address to use in identifying future retailers.

```{r}
address<-stores_newlist_allyr %>%
  ungroup() %>%
  select(addr_num,addr_st,city,state,zip5) %>%
  distinct() 

locid1<-data_frame(1:nrow(address))
names(locid1)<-"locid1"

address<-address %>%
  bind_cols(locid1) %>%
  mutate(loc_id=paste("L",stringr::str_pad(locid1,6, side="left", pad="0"),sep="")) %>%
  select(-locid1)

stores_newlist_allyr<-stores_newlist_allyr %>%
  left_join(address)

write_csv(stores_newlist_allyr,"data/snap_retailers_usda.csv")
```

